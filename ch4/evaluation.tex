\chapter{Evaluation}\label{ch:eval}

In this chapter, we will purpose some evaluation approaches regarding to both API layer emulation and ABI layer emulation approaches. Despite the fact that the real implementation and the benchmark haven't been finished yet we can still set up some criteria in terms of the implementation in both a qualitative aspect and a quantitative aspect. 

\section{Qualitative Evaluation} 

One way to evaluate whether the implementation of the emulation approaches is successful or not is to test the emulation framework with some existing seL4 applications. In this project, we are going to use applications from the following source:

\begin{itemize}
    \item seL4 tutorials
    \item seL4 test suit
    \item SOS(simple operating system)
\end{itemize}

The seL4 tutorials contain several trivial seL4 applications used for demonstration and education purposes.

Those applications mainly focus on demonstrating seL4 features and API usage including printing "Hello World", simple IPC message passing, setting up runnable threads in seL4 userland, etc. Those applications can be used to use to test each of the emulated seL4 features separately.

The seL4 test suit contains several unit tests that focus on low-level kernel APIs. As those tests were used to prove the correctness of the implementation of the real seL4 kernel and some seL4 helper libraries. Therefore, we can use those tests to evaluate the implementation of the seL4 kernel simulator. However, since the seL4 kernel simulator fully running in the Linux userspace, it doesn't have the access to the hardware and performs privilege options. We don't expect the kernel simulator can pass all of the tests. We will focus more on testing critical functionalities, such as the IPC mechanisms, cspace management. vspace management and scheduling, etc, and skip those tests that are not quite important to the emulation project or those are complex or even impossible to emulate correctly such as cache tests, multicore tests, etc.

The SOS is a simple multitasking operating system running on the seL4 kernel which has an interactive shell, and several subsystems including I/O, memory management, process management, etc. Since it has a relatively complex structure and more comprehensive functionalities implemented with seL4 semantics. Hence, emulating it using the emulation approach can be a nice way to both verify and demonstrate the emulation approaches function well.

\section{Quantitative Evaluation}

From the qualitative perspective, we are going to evaluate the two approaches via microbenchmark and macrobenchmark. Table \ref{tb:expt} is the expected quantitative benchmarking outcome.

\begin{table}[]
\centering
\caption{Expected success criteria}
\label{tb:expt}
\resizebox{\textwidth}{!}{\begin{tabular}{lll}
\addlinespace[0.2cm] 
 &
  syscall API emulation (A1) &
  kernel ABI emulation (A2) \\
  \addlinespace[0.2cm]
\toprule
\begin{tabular}[c]{l}On the emulated \\ hardware by QEMU (B1)\end{tabular} &
  \begin{tabular}[c]{l}A1 should be significantly \\ faster than B1 because no \\ hardware emulation is involved\end{tabular} &
  \begin{tabular}[c]{l}A2 should be significantly faster \\ than B1 but slower than A1 due \\ to syscall interception\end{tabular} \\
  \addlinespace[0.2cm]
  
\begin{tabular}[c]{l}Native hardware running \\ seL4 (B2)\end{tabular} &
  \begin{tabular}[c]{l}A1 should be less than an OoM \\ slower than B2 because seL4 is \\ highly optimized\end{tabular} &
  \begin{tabular}[c]{l}A2 should be significantly \\ slower than B2 due to syscall \\ interception\end{tabular}

\end{tabular}}
\end{table}

\subsection{Microbenchmark}

The microbenchmark mainly focuses on evaluating the performance of each emulated system call. We can measure the number of system calls that the seL4 application invokes per second. The higher the result, the better the performance. Besides, to make the result more representative, we should measure the result several times and make a statistical analysis.

The ideal expectation is listed in the table \ref{tb:expt}. In general, we expect the API emulation approach will not introduce too much overhead, and the result should be slower compared to the result of running the same application on the native seL4 kernel. In the worst case, the overhead upper bound shouldn't exceed an order of magnitude. While comparing to the result of using ABI layer emulation, hardware virtualization, or emulation approach, the API layer emulation should be faster. This is because, in the API layer approach, all the real seL4 system calls are emulated, which can save some of the overhead of trapping into the kernel.  

On the other hand, the ABI layer emulation approach should be faster than using any hardware emulation or virtualization, but slower than the API layer emulation approach, because intercepting system calls will introduce relatively high overhead.   

\subsection{Macrobenchmark}

The macrobenchmark can be commenced under two circumstances and the result will be the measurement of the total system calls' elapsed time. On one hand, we can simulate a situation with a heavy system call workload. The result can emphasize the expectation that using either API or ABI layer emulation approach will not introduce unacceptable overhead. On the other hand, we are going to simulate another situation with a low system call workload, the result emphasizes that using API or ABI emulation approach is more performant than using hardware virtualization or emulation approaches such as QEMU.